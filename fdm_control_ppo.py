# -*- coding: utf-8 -*-
"""FDM_Control PPO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iYiwWuGzngfUDKUlej8TKJe31GBQpG5U
"""

import sympy
import gymnasium as gym
from gymnasium import spaces
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.callbacks import BaseCallback
from gymnasium.wrappers import FlattenObservation

class TrainingCurveCallback(BaseCallback):
    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.step_rewards = [] # 이름 변경
        self.timesteps = []

    def _on_step(self) -> bool:
        # 조건문 없이, 매 스텝마다 '방금 받은 보상'을 즉시 저장합니다.
        current_step_reward = self.locals['rewards'][0]

        self.step_rewards.append(current_step_reward)
        self.timesteps.append(self.num_timesteps)

        return True

    def plot_results(self):
        plt.figure(figsize=(12, 4)) # 점이 많아지므로 그래프 가로 길이를 조금 늘림
        plt.plot(self.timesteps, self.step_rewards, label='Step Reward', color='blue', alpha=0.2)

        # 스텝 단위로 바뀌면 데이터가 수만 개가 되므로 이동 평균(Moving Average) 구간을 늘려줍니다.
        # 예: 최근 300스텝 (약 10 에피소드 분량)의 평균
        window_size = 300
        if len(self.step_rewards) > window_size:
            avg = np.convolve(self.step_rewards, np.ones(window_size)/window_size, mode='valid')
            plt.plot(self.timesteps[window_size-1:], avg, color='red', linewidth=2, label=f'Moving Average ({window_size} steps)')

        plt.xlabel('Total Timesteps')
        plt.ylabel('1-Step Reward (Max 30.0)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()

# # --- Callback function for training curve ---
# class TrainingCurveCallback(BaseCallback):
#     def __init__(self, verbose=0):
#         super().__init__(verbose)
#         self.rewards = []
#         self.timesteps = []
#         self.curr_reward = 0

#     def _on_step(self) -> bool:
#         self.curr_reward += self.locals['rewards'][0]
#         if self.locals['dones'][0]:
#             self.rewards.append(self.curr_reward)
#             self.timesteps.append(self.num_timesteps)
#             self.curr_reward = 0
#         return True

#     def plot_results(self):
#         plt.figure(figsize=(10, 4))
#         plt.plot(self.timesteps, self.rewards, label='Episode Reward', color='blue', alpha=0.3)
#         if len(self.rewards) > 10:
#             avg = np.convolve(self.rewards, np.ones(10)/10, mode='valid')
#             plt.plot(self.timesteps[9:], avg, color='red', linewidth=2, label='Moving Average (10 eps)')
#         plt.xlabel('Total Timesteps')
#         plt.ylabel('Reward')
#         plt.legend()
#         plt.grid(True, alpha=0.3)
#         plt.show()

# --- Environment set up ---
class DarcyEnv(gym.Env):
    def __init__(self):
        super(DarcyEnv, self).__init__()

        self.nx, self.ny = 16, 16
        self.L = 500.0

        self.dx = self.L / self.nx
        self.dy = self.L / self.ny

        self.bar = 1e5
        self.day = 24 * 60 * 60

        self.k = 100 * 1.0e-15         # 100 md
        self.phi = 0.2
        self.mu = 1.0e-3               # 1 cp
        self.ct = 1.0e-10              # 1e-5 / bar
        self.h = 10.0                  # Reservoir thickness (10m for 2D)

        self.alpha = self.k / (self.phi * self.mu * self.ct)

        self.dt_sim = 0.25 * (self.dx**2) / self.alpha * 0.9
        self.dt_ctrl = 1.0 * self.day
        self.n_substeps = int(self.dt_ctrl / self.dt_sim)

        self.s_max = 170.0 * self.bar

        self.q_max_m3_per_day = 200.0
        self.v_grid = self.dx * self.dy * self.h

        # Peaceman Well Index (WI)
        self.rw = 0.1 # 우물 반경 (10cm)
        self.req = 0.14 * np.sqrt(self.dx**2 + self.dy**2) # 범용 등가 격자 반경
        self.WI = (2 * np.pi * self.k * self.h) / (self.mu * np.log(self.req / self.rw))

        self.C_sink = (self.WI * self.dt_sim) / (self.v_grid * self.phi * self.ct)

        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.nx, self.ny), dtype=np.float32)

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        base = np.random.uniform(95, 105) * self.bar
        self.p = np.ones((self.nx, self.ny), dtype=np.float32) * base
        self.steps_taken = 0

        obs = ((self.p - 100*self.bar) / (100*self.bar)).astype(np.float32)
        return obs, {}

    def step(self, action):
        u_norm = float(action[0])

        q_inj_day = 100 + u_norm * (self.q_max_m3_per_day-100)
        q_inj_sec = q_inj_day / self.day

        delta_p_inj = (q_inj_sec * self.dt_sim) / (self.v_grid * self.phi * self.ct)

        curr_s = self.p.copy()
        sx = self.alpha * self.dt_sim / self.dx**2
        sy = self.alpha * self.dt_sim / self.dy**2

        p_bhp = 90.0 * self.bar

        early_terminated = False

        for _ in range(self.n_substeps):
            p_xx = curr_s[2:, 1:-1] - 2*curr_s[1:-1, 1:-1] + curr_s[:-2, 1:-1]
            p_yy = curr_s[1:-1, 2:] - 2*curr_s[1:-1, 1:-1] + curr_s[1:-1, :-2]
            curr_s[1:-1, 1:-1] = curr_s[1:-1, 1:-1] + sx*p_xx + sy*p_yy

            curr_s[1, 1] += delta_p_inj

            if curr_s[-2, -2] > p_bhp:
                curr_s[-2, -2] = (curr_s[-2, -2] + self.C_sink * p_bhp) / (1 + self.C_sink)

            curr_s = np.maximum(curr_s, 0)

            curr_s[0, :], curr_s[-1, :] = curr_s[1, :], curr_s[-2, :]
            curr_s[:, 0], curr_s[:, -1] = curr_s[:, 1], curr_s[:, -2]

            # If exceed 200 bar, terminate immediately
            if curr_s.max() > 200 * self.bar:
                early_terminated = True
                break

        self.p = curr_s.copy()
        self.steps_taken += 1

        max_p = self.p.max()

        # reward function
        r_control = (u_norm) * 30.0
        r_penalty = 0.0
        r_terminal = 0.0

        if max_p > self.s_max:
            r_penalty = -2.0 * (max_p - self.s_max) / self.bar

        terminated = False
        if self.steps_taken >= 30:
            terminated = True

        if early_terminated or max_p > 200 * self.bar:
            terminated = True
            r_terminal = -50.0

        total_reward = r_control + r_penalty + r_terminal

    # def step(self, action):
    #     u_norm = float(action[0])

    #     q_inj_day = u_norm * self.q_max_m3_per_day
    #     q_inj_sec = q_inj_day / self.day

    #     # injection pressure
    #     delta_p_inj = (q_inj_sec * self.dt_sim) / (self.v_grid * self.phi * self.ct)

    #     curr_s = self.p.copy()
    #     sx = self.alpha * self.dt_sim / self.dx**2
    #     sy = self.alpha * self.dt_sim / self.dy**2

    #     p_bhp = 90.0 * self.bar  # Sink BHP

    #     for _ in range(self.n_substeps):
    #         p_xx = curr_s[2:, 1:-1] - 2*curr_s[1:-1, 1:-1] + curr_s[:-2, 1:-1]
    #         p_yy = curr_s[1:-1, 2:] - 2*curr_s[1:-1, 1:-1] + curr_s[1:-1, :-2]
    #         curr_s[1:-1, 1:-1] = curr_s[1:-1, 1:-1] + sx*p_xx + sy*p_yy

    #         curr_s[1, 1] += delta_p_inj

    #         if curr_s[-2, -2] > p_bhp:
    #             curr_s[-2, -2] = (curr_s[-2, -2] + self.C_sink * p_bhp) / (1 + self.C_sink)

    #         curr_s = np.maximum(curr_s, 0)

    #         # Neumann BC (No-flow)
    #         curr_s[0, :], curr_s[-1, :] = curr_s[1, :], curr_s[-2, :]
    #         curr_s[:, 0], curr_s[:, -1] = curr_s[:, 1], curr_s[:, -2]

    #     self.p = curr_s.copy()
    #     self.steps_taken += 1

    #     max_p = self.p.max()

    #     # Reward function
    #     r_control = (u_norm ** 2) * 20.0
    #     r_penalty = 0.0
    #     r_terminal = 0.0

    #     if max_p > self.s_max:
    #         r_penalty = -2.0 * (max_p - self.s_max) / self.bar

    #     terminated = False
    #     if self.steps_taken >= 30:
    #         terminated = True
    #     if max_p > 200 * self.bar:
    #         terminated = True
    #         r_terminal = -20.0

    #     total_reward = r_control + r_penalty + r_terminal

        info = {
            "max_pressure": float(max_p/self.bar),
            "u": float(u_norm),
            "field": self.p.copy(),
            "r_control": r_control,
            "r_penalty": r_penalty,
            "r_terminal": r_terminal,
            "total_reward": total_reward
        }
        obs = ((self.p - 100*self.bar) / (100*self.bar)).astype(np.float32)

        return obs, float(total_reward), terminated, False, info

# --- Run Training ---
env = DarcyEnv()
env = FlattenObservation(env)
check_env(env)

callback = TrainingCurveCallback()
model = PPO("MlpPolicy", env, verbose=0, learning_rate=0.0003, ent_coef=0.01)

print("Training Start (Fast Mode)...")
model.learn(total_timesteps=20000, callback=callback)
print("Finished!")

callback.plot_results()

# --- Test code ---
n_test = 10
all_u, all_p = [], []
first_ep_frames = []
print(f"Testing {n_test} episodes and printing rewards...")

for i in range(n_test):
    print(f"\n--- Episode {i+1} ---")
    obs, _ = env.reset()
    done = False
    temp_u, temp_p = [], []
    step_count = 1

    if i == 0: first_ep_frames.append(env.unwrapped.p.copy())

    while not done:
        action, _ = model.predict(obs, deterministic=True)
        obs, _, terminated, truncated, info = env.step(action)
        done = terminated or truncated

        temp_u.append(info['u'])
        temp_p.append(info['max_pressure'])

        if step_count % 5 == 0 or done:
            print(f"Step {step_count:2d} | u: {info['u']:.3f} | Max P: {info['max_pressure']:.1f} bar | "
                  f"Total Reward: {info['total_reward']:.3f}")

        if i == 0: first_ep_frames.append(info['field'])
        step_count += 1

    while len(temp_u) < 30:
        temp_u.append(temp_u[-1])
        temp_p.append(temp_p[-1])
    all_u.append(temp_u)
    all_p.append(temp_p)

arr_u, arr_p = np.array(all_u), np.array(all_p)
mean_u, mean_p = arr_u.mean(0), arr_p.mean(0)

#------Visualize---------
print("Plotting Pressure Field Heatmaps...")
days_to_plot = [1, 2, 3, 4, 5]

fig, axes = plt.subplots(1, 5, figsize=(20, 5))

for idx, ax in enumerate(axes):
    day = days_to_plot[idx]
    frame_idx = min(day, len(first_ep_frames) - 1)
    field_bar = first_ep_frames[frame_idx] / 1e5

    im = ax.imshow(field_bar.T, cmap='jet', origin='lower', extent=[0, 500, 0, 500], vmin=90, vmax=190)

    ax.set_title(f'Day {day}')
    ax.set_xlabel('x (m)')
    if idx == 0: ax.set_ylabel('y (m)')

fig.colorbar(im, ax=axes.ravel().tolist(), label='Pressure (bar)')
plt.suptitle('Pressure Field Evolution (RL Controlled)', fontsize=16)
plt.show()

print("Plotting Statistical Graphs...")
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)
steps = range(1, 31)

for i in range(n_test): ax1.plot(steps, arr_u[i], 'r-', alpha=0.1)
ax1.plot(steps, mean_u, 'r-', linewidth=3, label='Mean U')
ax1.set_ylabel('Injection u'); ax1.legend(); ax1.grid(True, alpha=0.3)

for i in range(n_test): ax2.plot(steps, arr_p[i], 'b-', alpha=0.1)
ax2.plot(steps, mean_p, 'b-', linewidth=3, label='Mean P')
ax2.axhline(170, color='k', linestyle='--', label='Limit (170)')
ax2.set_ylabel('Max Pressure'); ax2.legend(); ax2.grid(True, alpha=0.3)
plt.show()